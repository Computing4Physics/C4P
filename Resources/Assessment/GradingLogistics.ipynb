{
 "metadata": {
  "name": "",
  "signature": "sha256:f9ca6c3082166601144c85f9278217b18b64bd4af0aecfa574d6c38259e7feea"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<img src=\"../../C4PLogo.png\" width=300 style=\"display: inline;\"> Grading Logistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Figuring out how to efficiently grade students' assignments is a non-trivial task.  Grading can be made more efficient by automatic output checking but that doesn't leave room for quality assessment and feedback.  To deal with the logistics of grading a set of scripts and grading practices has been developed.  They are outlined here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Scripts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* [cloneRepos.sh](scripts/cloneRepos.sh) - uses a text file with student info to clone repositories from github.\n",
      "* [updateRepos.sh](scripts/updateRepos.sh) - uses the text file with student info to pull updates of all repositories from github.\n",
      "* [makeGradeNotebook.sh](scripts/makeGradeNotebook.sh) - uses the text file with student info to create a blank grading notebook for a given assignment.  Each student has two notebook cells created for them - one for recording scores and comments that are saved to a text file, the other for reviewing the git log history to determine the date/time the assignment was turned in.  The text files are named according to `LastName_AssignmentName.txt` and saved in the student's repository directory.\n",
      "* [makeProjectGradebook.sh](scripts/makeProjectGradebook.sh) - similar to makeGradeNotebook.sh but specialized for the project grading with rubric categories and descriptions.\n",
      "* [emailGrades.sh](scripts/emailGrades.sh) - uses the text file with student info to generate personalized emails to each student for a given assignment with the grade text file created with the Grade Notebook appended to the email.  This is the method of communicating scores and comments back to the students.  The script must be executed on a machine with an email server.\n",
      "* [printGrades.sh](scripts/printGrades.sh) - extracts the total score for each student from the grade files for review and/or transfer to the gradebook."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The text file with student info has columns to represent Section #, Last Name, First Name, email alias, Github user account, repository name, major, class standing. For example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat scripts/student-info.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2\tBlow\t\tJoe\t\tjblow\t\tjblow202\tPHYS202-S14\tPHYS\tSophomore\r\n",
        "3\tDoe\t\tJane\t\tjdoe03\t\tjdoe202\t\tPHYS202-S14\tPHYS\tSophomore\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Homework Submission"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Students are required to \"turn in\" assignments by committing their changes to their local git repository and then pushing them to Github by the assignment deadline."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Basic Rubric"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The rubrics for grading can be found in the [syllabus](../Syllabus/Syllabus-2014.ipynb#Grading-Rubric)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Grader Instructions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are some general instructions on the workflow for grading."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. Use the script to clone/update student repositories.\n",
      "2. Create Grade Notebook using script.\n",
      "3. Open student's assignment notebook and evaluate based on criteria set for that assignment.\n",
      "4. Include comments and score for each element in the file magic code cell of the grade notebook, formatted according to assignment grading instructions.\n",
      "5. Check the git log for the assignment in the git code cell to verify that they turned it in by the deadline, typically 11:59pm on the due date.  If they submitted it within 4 hours after the due date, accept it without penalty.  If it is later than that, indicate in the text file with a -1 point penalty per day for the assignment. (e.g., if an assignment is worth 5 points or 20 points because it has 4 questions, just deduct 1 point per day late overall)\n",
      "6. Look for suspicious solutions that might have been copied verbatim from an external source.  In general, solutions to simple problems *are* likely to be similar.  Keep an eye out for any that have telling details, such as exact same wording, structure, comments, variable names, etc. that seem too similar to be a coincidence.  Keep track of these and flag as potential violations of the [academic honesty guidelines](../Syllabus/Syllabus-2014.ipynb#Guidelines-for-Academic-Honesty).  Follow up with student to correct behavior and/or take approriate action.  See point 7 for reasonable exceptions.\n",
      "7. Some assignments are done as \"pair-programming\" exercises where one person \"drives\" while the other \"navigates\" and they solve the problem together.  In these instances, it is mandatory that they indicate the pairing in their solution with a markdown cell indicating who is the driver and who is the navigator.  In that case, code similarities are to be expected and these need not be flagged for review.\n",
      "8. Use the printGrades script to record the grades in the grading spreadsheet."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Project Euler Grading Guidelines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For Project Euler assignments, the grading is fairly straightforward.  For full credit (5/5), they must have all of the following:\n",
      "\n",
      "1. Header Cell with Problem Title\n",
      "2. Markdown Cell with Problem Description\n",
      "3. Solution of the test case in the Problem Description (if provided)\n",
      "4. Solution of the problem\n",
      "\n",
      "Some example grades for specific cases, which can be indicated in the comments to the student:\n",
      "\n",
      "* Correct solution but missing 1 and/or 2 : 4/5\n",
      "* Correct solution but missing 3 : 3/5\n",
      "* Correct solution but missing 1 and/or 2 and 3 : 2/5\n",
      "* Incorrect solution but solid attempt (regardless of 1 or 2): 2/5\n",
      "* Incorrect solution because very little attempt (regardless of 1 or 2): 1/5\n",
      "* Did 1 and/or 2 but otherwise no solution : 1/5\n",
      "* No solution submitted : 0/5\n",
      "\n",
      "Include any comments about why their code might not be working.  E.g. if they left it because they\n",
      "got some kind of error and couldn't figure it out, offer a suggestion for what they did wrong.\n",
      "If their code runs but returns the wrong solution, try to figure out why and indicate what the problem is.  If it is too much work to figure out what they did wrong, just indicate that in the comments."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "NumPy Grading Guidelines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Guidelines provided to a grader for the NumPy exercises - a basic grading rubric."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are four questions, but some of them have subparts.  To reinforce **Learn by Doing**, they get full credit for doing the parts that don't require additional input beyond correctly typing in the code and executing it.\n",
      "\n",
      "**Question 1.**\n",
      "\n",
      "Give 5 points for array 1 and 5 points for array 2.  Maximum credit if they were able to achieve the result in just a few lines, similar to my examples.\n",
      "\n",
      "* 3/5 for brute force method but otherwise correct\n",
      "* 4/5 for getting the type of the array elements incorrect\n",
      "* 2/5 for multiple problems, such as brute force and a type error\n",
      "* 1/5 for a weak attempt\n",
      "* 0/5 for no attempt\n",
      "\n",
      "**Question 2.**\n",
      "\n",
      "There are a couple of ways to accomplish this one.  I gave two examples in my solutions.  Full credit if their solution involves using the NumPy tile method.\n",
      "\n",
      "* 2/5 for brute force, not using np.tile\n",
      "\n",
      "**Question 3.**\n",
      "\n",
      "* (a) 2/2 for executing the code I gave them\n",
      "* (b) 2/2 for executing the code I gave them\n",
      "* (c) 2/2 for executing the code I gave them\n",
      "* (d) 3/3 for correct implementation of np.nonzero\n",
      "\n",
      "* (e) This one is where they had to do some work:\n",
      "\n",
      "   * 5/5 for using NumPy arrays to create the sieve and print the primes with np.nonzero\n",
      "   * 3/5 for using Python lists instead of NumPy arrays\n",
      "   * Deduct a point if they didn't do both cases (10 and 100)\n",
      "   * For incorrect programs, use your discretion to decide whether their work merits an \"A-\" (4/5), a \"B\" (3/5), a \"C\" (2/5) or a \"D\" (1/5).\n",
      "   * Reserve 0/5 for those cases where there was no attempt.\n",
      "\n",
      "**Question 4.**\n",
      "\n",
      "* (a) 2/2 for executing the code I gave them\n",
      "* (b) 2/2 for executing the code I gave them\n",
      "* (c) 2/2 for executing the code I gave them\n",
      "* (d) 2/2 for executing the code I gave them\n",
      "\n",
      "* (e) This one is where they had to do some work:\n",
      "\n",
      "   * 5/5 Correct result\n",
      "   * 4/5 A common mistake may be using integers in the mask expression that produces very blocky output instead of a smooth curve.\n",
      "\n",
      "   * Same approximate rubric as question 3(e) for other types of incorrect answers.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise Solutions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These are \"available by request\" to authorized educators with a valid `.edu` email address.  Contact the [repository owner](mailto:jklay@calpoly.edu) to request them."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Project Philosophy and Assessment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I purposely choose projects that I have not personally coded myself that form a basis for answering real research questions.  There are several reasons I do this:  \n",
      "\n",
      "* I find it much more interesting to learn something new through the students from a course such as this.  I would be bored otherwise.\n",
      "* Having the students work on a novel project is similar to how I work with students in research mentoring.  My interactions with them are much more like a real research environment.  I can offer guidance and suggestions but my not \"knowing\" the answer to every problem or roadblock they encounter means I won't have to resist the temptation to give them the quick fixes.\n",
      "* These projects could easily be extended into senior projects or research internship opportunities, giving the students the motivation to keep working on the problem after the course is over.\n",
      "\n",
      "For these reasons, the project assessment is built less on \"correctness\" than on their formulation of the solution, documentation of the results, and their attempt to answer the assigned \"research question\".  The rubric is set up so that they can get most of the points for developing an organized, complete project with documentation, even if their results are incorrect.\n",
      "\n",
      "See the syllabus for [project grading guidelines](../Syllabus/Syllabus-2014.ipynb#Project-Grading)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"../../C4PLogo.png\" width=200 style=\"display: inline;\"> All content is under a modified MIT License, and can be freely used and adapted.  See the full license text [here](../../LICENSE)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}